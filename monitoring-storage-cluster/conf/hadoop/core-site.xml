<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration xmlns:xi="http://www.w3.org/2001/XInclude">

<!-- i/o properties -->

  <property>
    <name>io.file.buffer.size</name>
    <value>131072</value>
    <description>
        The size of buffer for use in sequence files. The size of this
        buffer should probably be a multiple of hardware page size
        (4096 on Intel x86), and it determines how much data is
        buffered during read and write operations.
    </description>
  </property>

  <property>
    <name>io.serializations</name>
    <value>org.apache.hadoop.io.serializer.WritableSerialization</value>
  </property>

  <property>
    <name>io.compression.codecs</name>
    <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec</value>
    <description>
        A list of the compression codec classes that can be used
        for compression/decompression.
    </description>
  </property>

  <property>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
    <description>The implementation for lzo codec.</description>
  </property>

<!-- file system properties -->

  <property>
    <name>fs.default.name</name>
    <!-- cluster variant -->
    <value>hdfs://monitoring01:8020</value>
    <description>
        The name of the default file system.  Either the
        literal string "local" or a host:port for NDFS.
    </description>
    <final>true</final>
  </property>

  <property>
    <name>fs.inmemory.size.mb</name>
    <value>256</value>
  </property>

  <property>
    <name>fs.trash.interval</name>
    <value>360</value>
    <description>
        Number of minutes between trash checkpoints.
        If zero, the trash feature is disabled.
    </description>
  </property>

  <property>
    <name>fs.checkpoint.dir</name>
    <value>/grid1/hadoop/hdfs/snn,/grid2/hadoop/hdfs/snn,/grid3/hadoop/hdfs/snn</value>
    <description>
        Determines where on the local filesystem the DFS secondary
        name node should store the temporary images to merge.
        If this is a comma-delimited list of directories then the image is
        replicated in all of the directories for redundancy.
    </description>
  </property>

  <property>
    <name>fs.checkpoint.edits.dir</name>
    <value>${fs.checkpoint.dir}</value>
    <description>
        Determines where on the local filesystem the DFS secondary
        name node should store the temporary edits to merge.
        If this is a comma-delimited list of directoires then teh edits is
        replicated in all of the directoires for redundancy.
        Default value is same as fs.checkpoint.dir
    </description>
  </property>

  <property>
    <name>fs.checkpoint.period</name>
    <value>86400</value>
    <description>The number of seconds between two periodic checkpoints.</description>
  </property>

  <property>
    <name>fs.checkpoint.size</name>
    <value>2048000000</value>
    <description>
       The size of the current edit log (in bytes) that triggers a periodic
       checkpoint even if the fs.checkpoint.period hasn't expired.
    </description>
  </property>

  <property>
    <name>ipc.client.idlethreshold</name>
    <value>8000</value>
    <description>
        Defines the threshold number of connections after which
        connections will be inspected for idleness.
    </description>
  </property>

  <property>
    <name>ipc.client.connection.maxidletime</name>
    <value>30000</value>
    <description>
        The maximum time after which a client will bring down the
        connection to the server.
    </description>
  </property>

  <property>
    <name>ipc.client.connect.max.retries</name>
    <value>50</value>
    <description>Defines the maximum number of retries for IPC connections.</description>
  </property>

  <!-- Web Interface Configuration -->
  <property>
    <name>webinterface.private.actions</name>
    <value>false</value>
    <description>
        If set to true, the web interfaces of JT and NN may contain
        actions, such as kill job, delete file, etc., that should
        not be exposed to public. Enable this option if the interfaces
        are only reachable by those who have the right authorization.
    </description>
  </property>

  <!-- security -->

  <property>
    <name>hadoop.security.authentication</name>
    <value>simple</value>
  </property>

  <property>
    <name>hadoop.security.authorization</name>
    <value>false</value>
  </property>

  <property>
    <name>hadoop.security.auth_to_local</name>
    <value>
        RULE:[2:$1@$0]([jt]t@.*EXAMPLE.COM)s/.*/mapred/
        RULE:[2:$1@$0]([nd]n@.*EXAMPLE.COM)s/.*/hdfs/
        RULE:[2:$1@$0](hm@.*EXAMPLE.COM)s/.*/hbase/
        RULE:[2:$1@$0](rs@.*EXAMPLE.COM)s/.*/hbase/
        DEFAULT
    </value>
  </property>

  <property>
    <name>hadoop.proxyuser.hive.groups</name>
    <value>users</value>
  </property>

  <property>
    <name>hadoop.proxyuser.hive.hosts</name>
    <value>monitoring01</value>
  </property>

  <property>
    <name>hadoop.proxyuser.oozie.groups</name>
    <value>users</value>
  </property>

  <property>
    <name>hadoop.proxyuser.oozie.hosts</name>
    <value>monitoring01</value>
  </property>

  <property>
    <name>hadoop.proxyuser.hcat.hosts</name>
    <value>monitoring01</value>
  </property>

  <property>
    <name>hadoop.proxyuser.templeton.groups</name>
    <value>users</value>
  </property>

  <property>
    <name>hadoop.proxyuser.templeton.hosts</name>
    <value>monitoring01</value>
  </property>

  <property>
    <name>hadoop.proxyuser.HTTP.groups</name>
    <value>users</value>
  </property>

  <property>
    <name>hadoop.proxyuser.HTTP.hosts</name>
    <value>*</value>
  </property>

  <!-- /security -->
</configuration>
